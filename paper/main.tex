\documentclass[10pt]{article}

% Order is important in package loading
% Load math packages and set margins
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{fancyhdr, color, comment, graphicx, environ, bbm}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{capt-of}
\usepackage{longtable}
\usepackage{parskip}
\usepackage{tcolorbox}

% to squeeze more space in
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{2ex}{1ex}
\titlespacing{\subsubsection}{0pt}{2ex}{1ex}
    
% Libertine Font
\usepackage{libertine}
\usepackage{mweights}
\usepackage[bigdelims,cmintegrals,libertine,vvarbb]{newtxmath}
\usepackage{zi4}

\usepackage[T1]{fontenc}
\usepackage{hyphenat}
\usepackage{blkarray}
\usepackage{bbding}
\usepackage[labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{booktabs}
\usepackage{import}
\usepackage[subpreambles=false]{standalone}
\usepackage{placeins}
\usepackage{svg}

\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{matrix, positioning, automata, shapes.multipart}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{colorbrewer}

\usepackage[framemethod=TikZ]{mdframed}
\mdfsetup{
    linewidth=1.5pt,
    innertopmargin=\dimexpr5pt-\topskip\relax
}

% Load hyperref last
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{cleveref}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,      
  urlcolor=blue,
  citecolor=blue
}

\newcommand{\ourmethod}{\textit{AlleleMinima}\xspace}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{thmtools}
\usepackage{thm-restate}

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{observation}{Observation}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}

\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\exportabletcolorbox}[2]{
  \label{tcbox:#1}
  \begin{tcolorbox}
    #2
  \end{tcolorbox}
}

\newcommand{\henri}[1]{\textcolor{blue}{[#1]}}
\newcommand{\tree}{\mathcal{T}}

\usepackage{todonotes}

\title{
    A regression based approach to phylogenetic reconstruction 
    from multi-sample bulk DNA sequencing of tumors
}
\author{Henri Schmidt}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
    Bulk DNA sequencing of multiple cancer patient tissues provides the opportunity to 
    investigate and reconstruct the phylogeny of patient tumors. However, as 
    bulk DNA sequencing of tumor tissue captures thousands of cells from a heterogenous 
    mixture of distinct subpopulations, accurate reconstruction of the tumor phylogeny
    requires simultaneous deconvolutiona and inference of ancestral relationships, leading
    to computationally challenging optimization problems. Here, we develop a new
    approach to phylogenetic reconstruction from multi-sample bulk DNA sequencing data by 
    studying a thus far ignored regression sub-problem, hidden within the phylogenetic
    reconstruction task. In particular, we derive a new algorithm for the 
    regression sub-problem by exploiting the unique, combinatorial structure of the matrices
    appearing within the regression, obtaining both asymptotic and empirical improvements
    over state-of-the-art algorithms. Using our algorithm for this regression sub-problem,
    we develop a strikingly simple method for phylogenetic inference from multi-sample bulk 
    DNA sequencing data, \ourmethod, which accurately reconstructs tumor phylogenies 
    using up to hundreds of samples containing dozens of distinct sub-populations. Finally, we
    demonstrate that on both simulated and real data, \ourmethod outperforms existing approaches
    in terms of its ability to scale to hundreds of samples and reconstruction accuracy.
\end{abstract}

% ABSTRACT DRAFT #1:
%   Bulk DNA sequencing of multiple cancer patient tissues provides the opportunity to 
%   investigate the evolutionary history of patient tumors. However, as bulk DNA sequencing
%   of tumor tissue captures thousands of cells from a heterogenous mixture of 
%   distinct subpopulations, deconvolution is first necessary to accurately infer the 
%   evolutionary history. The standard approach to this deconvolution problem is to 
%   factor the observed variant allele frequency (VAF) matrix into a
%   clonal matrix and a matrix specifying the fraction of the subpopulations in each sample. 
%   Here, we study a related regression problem, the $\ell_1$ VAF regression problem, that appears 
%   when the clonal matrix is fixed. By exploiting the structure of the clonal
%   matrix, we solve the $\ell_1$ VAF regression problem with expected time complexity
%   $\mathcal{O}(n^{3/2})$, where $n$ is the size of the clonal tree corresponding to 
%   the clonal matrix. We show that our algorithm outperforms blackbox approaches based on
%   linear programming (LP) for this problem, even when using state of the art LP solvers such
%   as Gurobi and CPLEX. Finally, we use our algorithm to develop a new approach to the deconvolution
%   problem using techniques from stochastic optimization, which outperform existing approaches
%   in terms of both speed and accuracy, on simulated data.

\section{Background}

In this section, we describe the model of cancer evolution that is widely used 
\cite{malikic_clonality_2015, el-kebir_reconstruction_2015, deshwar_phylowgs_2015, 
satas_tumor_2017, myers_calder_2019, wintersinger_reconstructing_2022} 
in reconstructing the evolutionary history of tumors from bulk DNA-sequencing data. Within this model,
the evolution of cancer proceeds by the accumulation of somatic, single-nucleotide
mutations starting with a single, founder cell. 
In this model, subpopulations of cells with identical genotypes are referred to as \emph{clones}.
Importantly, we assume that each \emph{genomic locus} is mutated exactly
once during this evolutionary process, an assumption known as the \emph{infinite sites assumption}. We
represent the state of a genomic locus with a binary $\{0, 1\}$ variable, where a $0$ represents the absence 
of a mutation and $1$ represents the presence of a mutation. Thus, the genotype of a clone is binary vector 
$\{0, 1\}^n$, where $n$ is the total number of genomic loci.

Under these assumptions, the evolutionary history of a tumor is described as a rooted, phylogenetic tree 
where vertices correspond to clones and edges their ancestral 
relationships. The root of this tree is the founder cell, which contains a single mutated locus. 
Each edge of this tree is labeled by exactly one mutation, and this mutation is represented by its 
position $i$ in the genome. More formally, we represent the evolutionary history of a tumor by an 
\emph{$n$-clonal tree} \cite{el-kebir_reconstruction_2015}, which is defined as 
follows.

\begin{definition}
  A rooted tree $\,\tree = (V, E)$ on $n$ vertices is an $n$-clonal tree 
  for a mutation set $[n] = \{1, \ldots, n\}$ if each edge is
  labeled by exactly one mutation in $[n]$ and no mutation appears
  on more than one edge.
\end{definition}

The root $r(\tree)$ of a clonal tree $\tree$ is assigned the unique 
mutation that does not appear as a label on any edge and is denoted
$r$ when the tree is clear by context. The vertices and edges of $\tree$ are denoted as $V(\tree)$ and $E(\tree)$.
The parent of a non-root, vertex $i$ in $V(\tree)$ is written as $\delta(i)$ and the set of children of a vertex $i$ 
is written as $C(i)$. Associated with any $n$-clonal tree is an \emph{$n$-clonal matrix} \cite{el-kebir_reconstruction_2015}, 
which describes the genotype of each clone. 

\begin{definition}
    An $n$-by-$n$ binary matrix $\,B = B_{\tree}$ is the unique, $n$-clonal matrix associated with $\tree$ 
  if and only if $B_{i, j} = 1$ when $i$ is a descendant of $j$ in $\tree$ or 
  $i = j$.
\end{definition}

The model of bulk DNA sequencing that is used is then quite simple: each of $m$ samples 
consists of a mixture of distinct clones and the sequencing experiment measures the 
frequency of all mutations in this mixture. More formally, it is assumed that 
each of the $m$ measurements $f_i \in \mathbb{R}^n$ are \emph{convex combinations} of the clones
in $\tree$. That is,
\[f_i = u_{i,1}b_1 + u_{i,2}b_2 + \cdots + u_{i,n}b_n \quad\text{where}\quad u_{i, j} \geq 0 \quad\text{and}\quad \sum_{j=1}^nu_{i, j} \leq 1,\]
where $b_i$ corresponds to the genotype of clone $i$ in the clonal tree $\tree$ associated with $B$.
This model is often summarized compactly in matrix notation as $F = UB$, where $F = [f_i]$ and $U = [u_i]$ are $m$-by-$n$ matrices, 
$f_{i,j}$ is the frequency of mutation $j$ in sample $i$, and $u_{i,j}$ is the fraction of clone $j$ in sample $i$. As such, we
call $F$ a \emph{frequency matrix} and any right stochastic matrix $U$ a \emph{usage matrix}.

Under this model, the problem of reconstructing the evolutionary history of the tumor
becomes equivalent to factoring the observed frequency matrix $F$ into its constituents $U$ and $B$.
The \emph{variant allele frequency deconvolution problem} \cite{el-kebir_reconstruction_2015}
is one such formalism of this correspondence: given the observed frequency matrix $F$, 
the problem asks to find a usage matrix $U$ and a clonal matrix $B$ such that $F = UB$, or determine
if no such matrices exist. Unfortunately, the variant allele frequency deconvolution problem is 
NP-complete \cite{el-kebir_reconstruction_2015}, which is not too surprising as it is a difficult 
deconvolution problem over the discrete space of trees.

Here, we pose a generalization of the variant allele frequency deconvolution problem.
That is, rather than require that $F = UB$, we formalize the reconstruction problem as
requiring that the error $\epsilon := F - UB$ be small. This approach has
a couple of advantages: \textit{i)} real data contains error and it may not be possible to reconstruct
$F$ exactly and \textit{ii)} it allows for a notion of an approximate solution. This is also what is sometimes done in practice: 
e.g. AncesTree \cite{el-kebir_reconstruction_2015} relaxes the hard requirement that $F = UB$ and 
adds an $\ell_1$ regularization term $\lVert \epsilon \rVert_1$ to the objective.  
This is also the variant of the problem studied in CITUP \cite{malikic_clonality_2015}, which aims to 
minimize the $\ell_2$ objective $\lVert \epsilon \rVert_2$. 

\begin{problem}
  \label{prob:vafp}
  Given a frequency matrix $F$, \emph{variant allele frequency $\ell_p$-deconvolution problem} ($\ell_p$-VAFDP) is to
  find a both a usage matrix $U$ and a clonal matrix $B$ such that 
  \begin{equation}
    \sum_{i=1}^m\lVert F_i - (UB)_i \rVert_p
  \end{equation}
  is minimized.
\end{problem}

Of course, the computational hardness of the VAF deconvolution problem immediately implies the hardness of 
$\ell_p$-VAFDP since for any $\ell_p$-norm, the minimizer is zero if and only if $F = UB$. However, we 
again emphasize that a not insignificant notion of approximation appears when formalizing the problem in this 
manner. That is, rather than hoping for a perfect reconstruction, we accept one that is ``good enough.''

The main focus of this work is the regression problem that appears when the
clonal matrix $B$ (equivalently, the clonal tree $\tree$) is fixed. Specifically,
when $B$ is fixed, the problem reduces to finding a right stochastic matrix $U$
that minimizes $\lVert F - UB\lVert_p$ for some $\ell_p$-norm. For both the $\ell_1$ and $\ell_2$ norms, 
it is immediately apparent that this regression problem is solveable in polynomial time. 
More formally, since right stochasticity is enforcable by a set of linear constraints on the matrix $U$,
the $\ell_1$ and $\ell_2$ regression problems can be formulated as linear and quadratic programs, 
which are solveable in polynomial time \henri{cite}. 

\begin{problem}
  \label{prob:vafpp}
  Given a frequency matrix $F$ and a clonal matrix $B$, the 
  \emph{variant allele frequency $\ell_p$-regression problem} ($\ell_p$-VAFRP) is to
  find a usage matrix $U$ such that 
  \begin{equation}
    \sum_{i=1}^m\lVert F_i - (UB)_i \rVert_p
  \end{equation}
  is minimized.
\end{problem}

\henri{describe this better}
Throughout the remainder of the paper, we will work with the special case of 
the $\ell_p$-VAFRP where the number of samples $m = 1$. This is sufficient because both the objective 
function and the constraints on $U$ are separable in the coordinate 
$i$. Thus, we will refer to both $U$ and $F$ as row vectors, denoted as $u^T$ and $f^T$ 
respectively.

\henri{move into this}
Despite solving $\ell_p$-VAFRP in polynomial time for $p \in \{1, 2\}$, blackbox techniques based on 
linear and quadratic programming are still quite slow. For example, given a generic linear programming
problem of the form $\min_{Ax \leq b, x \geq 0} c^Tx$, with $n$ variables and $d$ constraints, the fastest
algorithm non-algebraic method \cite{cohen2021solving} takes 
$\mathcal{O}^*\left(\sqrt{d}\cdot\text{nnz}(A)+d^{2.5}\right)$ time \cite{lee2014path, lee2015efficient}. 
However, the linear programming formulation of $1$-VAFRP contains $n$ variables and 
$d = \mathcal{O}(n)$ constraints when $m = 1$, which implies that the fastest, non-algebraic method for this due to 
Vaidya \cite{vaidya1987algorithm, vaidya1996new}, takes $\mathcal{O}(n^{2.5})$ time. While algebraic methods
for matrix multiplication can boost the performance to $\mathcal{O}^*(n^{\omega})$ time where $\omega$ is
the matrix multiplication constant \cite{cohen2021solving}, which is currently $\omega \sim 2.37$ 
\cite{alman2021refined}. Further, the techniques for achieving this speedup, called the ``laser method,'' contain
impractically large constant factors \henri{cite} and are not used in practice, and instead rely on Strassen's algorithm \henri{cite}.
Finally, current state-of-the-art linear programming 
solvers \henri{cite gurobi and cplex} still employ classical algorithms such
as the simplex \henri{cite} and barrier \henri{cite} methods, many of which have exponential time 
worst case behavior.

\section{Theory}
Thus far, the main theoretical focus in inferring tumor phylogenies from bulk DNA sequencing data 
has centered around the computationally challenging, variant allele frequency \emph{deconvolution} 
problem \henri{cite}. Here, we complement this existing theory by shifting the focus 
towards the more tractable, variant allele frequency \emph{regression} problem. In 
particular, we start by studying the structure of clonal matrices, both summarzing 
and extending existing results. Then, narrowing our focus, we derive an equivalent
characterization of the variant allele frequency $\ell_1$-regression problem, which 
emphasizes the tree structure inherent in the problem. Finally, we use our
characterization to design an efficient algorithm for the variant allele 
frequency $\ell_1$-regression problem, providing the main theoretical result of 
our work, as stated below. 
\begin{restatable}{theorem}{ellonepolytime}
    \label{thm:ellonepolytime}
    Given a clonal tree $\tree$ with $n$ vertices and a frequency vector $f \in \mathbb{R}^n$, the
    minimum of 
    \[\left\lVert f - u^T B_{\tree}\right\rVert_1 \]
    over all usage vectors $u \in \mathbb{R}^n$ can be found in $\mathcal{O}(nd)$ where $d$ is 
    the diameter of $\tree$.
\end{restatable}

Importantly, the diameter $d$ of a tree on $n$ vertices is at most $n - 1$, implying our algorithm 
runs in quadratic time in the worst case and improving upon linear programming based approaches. 
However, for reasonable classes of trees, the complexity is much better. For example, the expected
diameter of a spanning tree drawn uniformly at random from the complete graph $K_n$ is 
of order $\sqrt{n}$ -- similar bounds can also be derived 
for the random spanning trees of an arbitrary graph $G$ \cite{renyi_height_1967, chung_diameter_2012}.
All proofs not included in this section can be found in
Supplementary Proofs \ref{sec:supplementary_proofs}.


\subsection{Clonal trees and matrices}
\label{sec:clonal-trees-matrices}

The most salient feature of clonal matrices is that they are (two-state) perfect phylogeny matrices 
\cite{el-kebir_reconstruction_2015}, and this allows us to tap into the rich theory 
which studies such matrices \henri{cite}.
However, the class of clonal matrices is much more restrictive:
there are a handful of useful results concerning clonal matrices 
which are not applicable to perfect phylogeny matrices.
For example, a perfect phylogeny matrix need
not be square and thus it is not necessarily invertible, while clonal matrices are 
always invertible \cite{el-kebir_reconstruction_2015}. 

We continue the study of clonal matrices by first drawing an analogy to perfect phylogeny. 
Specifically, we introduce a new, recursive definition
of clonal matrices that is inspired by the recursive definition of perfect
phylogeny matrices described by Gusfield \cite{gusfield_efficient_1991, peer_incomplete_2000}. Formally,
we show that a matrix $B$ is an $n$-clonal matrix if and only if
the rows and columns of $B$ can be reordered such that $B$ is \emph{clonally canonical},
a term we define below.

\begin{definition}
    A matrix $B$ is clonally canonical if i) $B_{i, 1} = 1$ for all $i \in [n]$,
    ii) $B_{1, j} = 0$ for all $j \in \{2, \ldots, n\}$, and iii) there exists 
    clonally canonical matrices $B_1, B_2, \ldots, B_k$ such that $B_{2:n , 2:n}$ is
    block diagonal with blocks $B_1, B_2, \ldots, B_k$. That is, $B$ is clonally canonical
    if it has the form:
    \[B= \left(\begin{array}{c|ccc}
        1 & 0 & \cdots & 0\\
        \hline
        1 & B_1 & & \mathbf{0} \\
        \vdots & & \ddots & \\
        1 & \mathbf{0} & & B_k \\
    \end{array}\right)\]
\end{definition}

Conveniently, for given a clonal tree $\tree$, it is straightforward to construct a clonally 
canonical matrix associated with $\tree$ by relabeling the clones by their preorder traversal 
index during a breadth first search starting at $r$. With this definition, we are now ready 
to state several equivalent characterizations of clonal matrices.

\begin{restatable}{proposition}{clonallycanonical}
    \label{prop:clonally_canonical}
    For any binary $n$-by-$n$ matrix $B$, the following conditions are
    equivalent:
    \begin{enumerate}[label=(\roman*)]
        \item $B$ is an $n$-clonal matrix.
        \item The rows and columns of $B$ can be reordered to make $B$ clonally canonical.
        \item $B$ satisfies the following conditions \cite{el-kebir_reconstruction_2015}:
            \begin{enumerate}
                \item There exists exactly one index $r$ such that $\sum_{j=1}^nB_{r, j} = 1$.
                \item For all $i \neq r$, there exists exactly one $j$ such that 
                    $I(j) \subseteq I(i)$ and $\sum_{k=1}^n\left(B_{i,k} - B_{j, k}\right)= 1$.
                \item $B_{i, i} = 1$ for all indices $i \in [n]$.
            \end{enumerate}
    \end{enumerate}
\end{restatable}
The clonally canonical form of the clonal matrix associated with a clonal tree $\tree$ is 
especially useful since it enables efficient multiplication and inversion of $B$. 

\begin{proposition}
    Let $B$ be an $n$-by-$n$ clonally canonical matrix associated with the clonal tree $\tree$. Then, the four matrix vector
    products \[Bv,\ v^TB,\ B^{-1}v \text{ and } v^TB^{-1}\] can be computed in $\mathcal{O}(n)$ 
    time for any vector $v \in \mathbb{R}^n$ when given the clonal tree $\tree$.
\end{proposition}

We conclude this section with an algebraic description of the
inverse of $B$, which was first described in \cite{jia_efficient_2018}.
Define the adjacency matrix $A$ of $\tree$ as the $n$-by-$n$ matrix 
such that $A_{i, j} = 1$ if $i$ is a parent of $j$ in $\tree$ and $0$ otherwise.
Then, the inverse of $B$ is $(I - A)$, where $I$ is the identity matrix.

\begin{lemma}[\cite{jia_efficient_2018}]
    \label{lemma:clonal_inverse}
    For any clonal matrix $B$ associated with a clonal tree $\,\tree$ with adjacency matrix $A$,
    we have that
    \[
        B = (I - A)^{-1} \quad\text{and}\quad [(I - A)v]_i = \begin{cases}
            v_i - v_{\delta(i)} &\ \text{if}\  i \neq r(\tree), \\
            v_i &\ \text{otherwise}.
        \end{cases}
    \]
\end{lemma}
\begin{proof}
    Observe that $A^k_{i,j} = 1$ if and only if $j$ is an ancestor of $i$ in $\tree$
    at tree distance $k$ from $i$. Then,
    \[B = A^0 + A^1 + A^2 + \cdots + A^{n} = I + A + A^2 + \cdots = (I - A)^{-1}\] 
    where the first equality follows from the observation above. The
    second equality from the observation that $A^k$ is the zero matrix when 
    $k > n$.
\end{proof}

\subsection{An equivalent formulation of the VAF 1-regression problem}
\label{sec:equivalent_formulations}

In this section, we show that the variant allele frequency 1-regression problem is 
equivalent to a constrained vertex labeling problem on the clonal tree $\tree$ associated with $B$.
Specifically, we prove that the $1$-VAF regression problem is equivalent to a special case of 
the \emph{dot product tree labeling problem}, defined as follows.
%labeling problem using a dynamic programming algorithm, which in turns solves the $1$-VAFRP.
\begin{problem}
\label{prob:dot_product_tree_labeling}
Given a rooted tree $\tree$ with vertices $[n] = \{1, \ldots, n\}$ and a vector $w \in \mathbb{R}^n$,
the \emph{dot product tree labeling problem} (DPTLP) is to
find a non-negative vector $x \in \mathbb{R}^n$ such that $x^Tw$ is maximized
and $\lvert x_i - x_{\delta(i)}\rvert \leq 1$ for all $i \neq r(\tree)$. 
\end{problem}

\begin{figure}
    \begin{tcolorbox}[colback=white,colframe=black]
        \begin{minipage}[t]{0.40\textwidth}
            \vspace{-1em}
            \begin{align}
                \max_{u \geq 0, z \geq 0} &-\sum_{i=1}^n z_i \nonumber \\
                \text{s.t.}\quad z_i &\geq f_i - \sum_{j=1}^n u_j B_{ji} \quad\text{for}\  i \in [n] \label{eq:constr1} \\
                z_i &\geq \sum_{j=1}^n u_j B_{ji} - f_i \quad\text{for}\  i \in [n] \label{eq:constr2} \\
                1 &\geq \sum_{i=1}^n u_i \label{eq:constr3}
            \end{align}
        \end{minipage}
        \hfill\vline\hfill
        \hspace{-2em}
        \begin{minipage}[t]{0.50\textwidth}
            \vspace{-1em}
            \begin{align}
                \min_{\alpha \geq 0, \beta \geq 0, \gamma \geq 0} \gamma + \sum_{i=1}^n f_i(\beta_i - \alpha_i) \nonumber \\
                \text{s.t.} \quad 
                \sum_{j=1}^n B_{ij}(\beta_j - \alpha_j) + \gamma &\geq 0 \quad\text{for}\  i \in [n] \label{eq:dualconstr0} \\
                \alpha_i + \beta_i &\leq 1 \quad\text{for}\  i \in [n] \label{eq:dualconstr1}
            \end{align}
        \end{minipage}
    \end{tcolorbox}
    \caption{\label{fig:primal_dual} The \textbf{(left)} primal and \textbf{(right)} dual linear 
    programs associated with the variant allele frequency 1-regression problem.}
\end{figure}

To derive the equivalence between the $1$-VAFRP and the DPTLP, we start by writing the 
$1$-VAFRP as a linear program in standard form \henri{cite}.
Using the usual trick \henri{cite} for converting the $\ell_1$ 
norm to a linear objective with linear constraints, we write the $1$-VAFRP as a 
linear program (Figure~\ref{fig:primal_dual}). Then, we write out the dual problem by associating
a dual variable $\alpha_i$ with the constraint in~\eqref{eq:constr1},
a dual variable $\beta_i$ with the constraint in~\eqref{eq:constr2},
and a dual variable $\gamma$ with the constraint in~\eqref{eq:constr3}.
Thus obtaining our dual linear program (Figure~\ref{fig:primal_dual}).

To simplify the dual form of the linear program, we perform a change of variables by setting 
$\lambda_i = \beta_i - \alpha_i$. Since $\alpha_i$ and $\beta_i$ are non-negative and their sum 
is bounded by $1$, and $\lambda_i \in [-1, 1]$. Then, writing the constraints in matrix form and 
using a slack variable to remove the inequality constraint, we have
the following equivalent, dual linear program. 
\vspace{-0.5em}
\begin{align}
    \min_{\gamma \geq 0, \psi \geq 0} \gamma + f^T\lambda \nonumber \\
    \text{subject to } \quad 
    B\lambda &= \psi - \gamma\mathbbm{1} \label{eq:dualconstr0} \\
    \lambda_i &\in [-1, 1] \quad\text{for}\  i \in [n]
\end{align}
Applying Lemma \ref{lemma:clonal_inverse} to the matrix $B$ in~\eqref{eq:dualconstr0} and invoking
strong linear programming duality \henri{cite} then proves the following theorem.
\begin{theorem}
\label{thm:dptlp_equivalence}
  Given a length $n$ frequency vector $f$ and an $n$-by-$n$ clonal matrix $B$
  corresponding to a clonal tree with root vertex 0,
  the minimum of
  \[\lVert f^T - u^TB \rVert_1\]
  over all usage vectors $u \in \mathbb{R}^n$ is equal to the maximum of
  \[
      w^Tx \quad\text{s.t.}\quad
      w_i = \begin{cases}
          f_{i} - 1 &\text{if } i = n + 1,\\
          \sum_{j \in C(i)} f_j - f_{i} &\text{otherwise.}
    \end{cases}
  \]
  %/of $$\gamma(f_{0} - 1) + \sum_{i=1}^n \psi_i\left(\sum_{j \in C(i)} f_j - f_i\right)$$
  over the  non-negative vector $x \in \mathbb{R}^{n+1}$ such that 
  $|x_0 - x_{n + 1}| \leq 1$ and $|x_i - x_{\delta(i)}| \leq 1$ for all $i \in [n]$.
\end{theorem}

In other words, the above theorem states that the $1$-VAF regression problem is equivalent to a special 
case of the dot product tree labeling problem where we append a parent labeled $n + 1$ to the root vertex and appropriately
set the vector $w \in \mathbb{R}^{n + 1}$. As a corollary to this equivalence, we extend the 
theory of El-Kebir \cite{el-kebir_reconstruction_2015} which states that we can find a usage vector 
$u^T$ such that $f^T = u^TB$ if and only if the \emph{sum condition} \cite{el-kebir_reconstruction_2015} is 
satisfied.

\begin{corollary}
    Let $f$ be a length $n$ frequency vector and $B$ be an $n$-by-$n$ clonal matrix.
    Then, the minimum of $\lVert f^T - u^TB \rVert_1$ over all usage vectors $u \in \mathbb{R}^n$
    is 0 if and only if the sum condition is satisfied and is otherwise
    greater than or equal to 
    \[\sum_{i=1}^n \max\left\{\sum_{j \in C(i)} f_j - f_i, 0\right\},\]
    the total violation of the sum condition.
\end{corollary}
\begin{proof}
    The first part of the corollary follows Lemma 4 of El-Kebir \cite{el-kebir_reconstruction_2015}. 
    The second part of the corollary follows from observing that 
    \[x_i = \begin{cases}
        1 &\text{if } \sum_{j \in C(i)} f_j - f_i > 0,\\
        0 &\text{otherwise.}
    \end{cases}\]
    is a feasible solution to the dot product tree labeling problem in Theorem \ref{thm:dptlp_equivalence}.
\end{proof}

\subsection{An algorithm for the dot product tree labeling problem}

In this section, we derive an efficient algorithm for the dot product tree labeling problem. 
There are two key ideas underlying our algorithm. The first idea is that 
exploiting the tree structure enables us to express the solution for the subtree rooted at a vertex $i$
in terms of the solution for the subtrees rooted at the children of $i$. This expression is derived using standard techniques 
for dynamic programming on trees \henri{cite}. The second idea is more technical, and is based on the observation
that the solution of this recurrence is a concave piecewise linear function, which we represent compactly as a list of size 
$\mathcal{O}(d)$, when $d$ is the depth of $\tree$. Taken together, these two ideas yield an $\mathcal{O}(nd)$ algorithm for the dot product tree 
labeling problem, which we describe in detail below.

Define $g_{i}(\psi)$ as the optimal solution to dot product tree labeling problem for the subtree rooted at $i$
when the root vertex $i$ is assigned the label $\psi$. Then, $g_{i}(\psi)$ satisfies the following recurrence relation:
\begin{align}
    g_{i}(\psi) &= \psi\cdot w_i + \max_{\substack{\lvert \psi_j - \psi \rvert \leq 1,\\\psi_j \geq 0}} \left\{\sum_{j \in C(i)} g_{j}(\psi_j)\right\} &&\text{by definition,} \nonumber\\
                &= \psi\cdot w_i + \sum_{j \in C(i)}\max_{\substack{\lvert \psi_j - \psi \rvert \leq 1,\\\psi_j \geq 0}} g_{j}(\psi_j) &&\text{by separability,} \nonumber\\
                &= \psi\cdot w_i + \sum_{j \in C(i)}h_j(\psi), &&\text{by defining } h_j(\psi) := \max_{\substack{\lvert \psi_j - \psi \rvert \leq 1,\\\psi_j \geq 0}} g_{j}(\psi_j).
\end{align}
Importantly, to solve the dot product tree labeling problem, it is necessary and sufficient 
to compute $\max_{\psi \geq 0}g_r(\psi)$ for the root vertex $r = r(\tree)$. Unfortunately, however,
the straightforward technique \henri{cite} of storing a dynamic programming table for this recurrence will not
work because the number of possible values of $\psi \geq 0$ is infinite. Thus, we need to find an alternative way to
represent and work with the functions $g_j$ and $h_j$ appearing in this recurrence.

To efficiently represent the functions $g_j$ and $h_j$, we use the
following representation of concave piecewise linear functions with
a finite number of breakpoints at coordinates $\{1, 2, \ldots, k\}$. 
\begin{definition}
    \label{def:g_form}
    We say that $f : \mathbb{R}^+ \rightarrow \mathbb{R}$ 
    is \emph{represented by the tuple} $(y, s_1, s_2, \ldots, s_{k + 1})$ if and only if $f$ is
    a concave piecewise linear function with intercept $y$ and 
    slopes $s_1 \geq s_2 \geq \ldots \geq s_{k + 1}$ on the intervals $[0, 1], [1, 2], \ldots, [k, \infty)$, respectively. 
\end{definition}
Of course, for this representation
to be useful, we will need to show that the functions $g_j$ and $h_j$ are indeed concave 
piecewise linear functions with breakpoints at coordinates in $\mathbb{N}$. 
To start, notice that if $i$ is a leaf vertex, then $g_{i}(\psi) = \psi\cdot w_i$ is 
a linear function represented by the tuple $(0, w_i)$. This provides us with the  
necessary base case required to prove that $g_j$ has the desired representation.

Next, we prove the inductive step of our claim: if $g_j$ has the desired representation, 
then $h_j$ does as well. Further,
we show the representation of $h_j$ is easily computed from the representation of $g_j$. 
We start with the following description of $h_j$ in terms of $g_j$.
\begin{restatable}{lemma}{hform}
\label{lemma:h_form}
Let $f$ be represented by the tuple $(y, s_1, s_2, \ldots, s_{k + 1})$. 
Let $i^*$ be the largest index $i$ such that $s_i \geq 0$. If $\,s_i < 0$ for all $i$, we set $i^* = -\infty$ and 
if $\,s_i \geq 0$ for all $i$, we set $i^* = \infty$. Then,
\[h(\psi) := \max_{\substack{\lvert \psi' - \psi \rvert \leq 1,\\\psi' \geq 0}} f(\psi')\] is the concave
piecewise linear function
\begin{equation}
    \label{eq:h_form}
    h(\psi) = \begin{cases}
        \,f(i^*) &\text{if } \psi \in [i^* - 1, i^* + 1],\\
        \,f(\psi + 1) &\text{if } \psi < i^* - 1,\\
        \,f(\max\{\psi - 1, 0\}) &\text{if } \psi > i^* + 1.
    \end{cases}
\end{equation}
\end{restatable}
With this description of $h_j$ in terms of $g_j$, we can quickly compute the representation of $h_j$ as 
follows.
\begin{proposition}
    \label{prop:h_form}
    Let $f$ be represented by the tuple $(y, s_1, s_2, \ldots, s_{k + 1})$. Let $i^*$ be the largest index $i$ such that $s_i \geq 0$. 
    If $\,s_i < 0$ for all $i$, we set $i^* = -\infty$ and if $\,s_i \geq 0$ for all $i$, we set $i^* = \infty$.
    Then, \[h(\psi) := \max_{\substack{\lvert \psi' - \psi \rvert \leq 1,\\\psi' \geq 0}} f(\psi')\]
    is represented by the tuple
    \begin{align*}
        (y + s_1, s_2, s_3, \ldots, s_k) \qquad&\text{if } i^* = \infty,\\
        (y, 0, s_1, s_2, s_3, \ldots, s_k) \qquad&\text{if } i^* = -\infty,\\
        (y + s_1, s_2, s_3, \ldots, s_{i^* - 1}, 0, 0, s_{i^* + 1}, s_{i^* + 2}, \ldots, s_k) \qquad&\text{otherwise}.
    \end{align*}
\end{proposition}

In summary, we have shown that \textit{i)} if $i$ is a leaf, $g_i$ is represented by the tuple $(0, w_i)$,
and \textit{ii)} if $g_j$ is a concave piecewise linear function, then $h_j$ is as well. The final 
step is to show that if the function $g_j$ is a concave piecewise linear function for all children $j$ of $i$,
then $g_i$ is as well. 
This follows from a basic observation: given the representations of $k$ functions $f_1, \ldots, f_k$, we
can efficiently compute the representation of their sum $f_1 + \cdots + f_k$.
\begin{observation}
    \label{obs:g_sum}
    Let $f$ be represented by the tuple $(y, s_1, s_2, \ldots, s_{k + l})$ and
    $h$ be represented by the tuple $(z, t_1, t_2, \ldots, t_{k})$ such that $l \geq 0$. 
    Then, $f + h$ is represented by the tuple
    \[(y + z, s_1 + t_1, \ldots, s_{k} + t_{k}, s_{k + 1} + t_{k}, \ldots, s_{k + l} + t_{k}).\]
\end{observation}
\vspace{-0.5em}
Further, the length of the tuple representing the sum $f + h$ is $\max\{k, k + l\} + 1 = k + l + 1$.
Following repeated application of the identity $\max\{a, b, c\} = \max\{\max\{a, b\}, c\}$,
this holds for the sum of $k$ functions $f_1, \ldots, f_k$ as well.
\begin{observation}
    \label{obs:g_sum_length}
    Let $f_1, \ldots, f_m$ be represented by tuples of length $k_1 + 1, \ldots, k_m + 1$, respectively. 
    Then, $f_1 + \cdots + f_m$ is represented by a tuple of length 
    $\max\{k_1, \ldots, k_m\} + 1$.
\end{observation}

Notice that the length of the tuple representing $h_j$ is at most $k + 1$, where $k$ is the length of the tuple representing $g_j$. 
Combined with Observation \ref{obs:g_sum_length}, this implies that the length of the tuple representing $g_i$ is at most $\text{height}(i)$,
the length of the longest path from vertex $i$ to a leaf in the subtree rooted at $i$. We summarize these
observations in the following proposition.
\begin{proposition}
    \label{prop:g_pieces}
    Let $g_i(\psi)$ be the optimal solution of the dot product tree labeling problem for the subtree rooted at 
    vertex $i$ such that $i$ is assigned the label $\psi$.
    Then, $g_i$ is a continuous piecewise linear function represented by a tuple of length at most $\text{height}(i)$.
\end{proposition}

We are now ready to prove the main result of this section.
\begin{theorem}
    \label{thm:dptlp_polytime}
    Given a rooted tree $\tree$ with vertices $[n] = \{1, \ldots, n\}$ and a vector $w \in \mathbb{R}^n$, the 
    dot product tree labeling problem can be solved in $\mathcal{O}(nd)$ time where $d$ is the depth of the tree $\tree$.
\end{theorem}
\begin{proof}
    The result follows by induction on $n$, the number of vertices in $\tree$. Clearly, the result holds for the base
    case of $n = 1$. Now, suppose the result holds for all trees with fewer than $n$ vertices.

    Let $r$ be the root of $\tree$ and let $C(r)$ be the set of children of $r$. Let $\tree_j$ denote the subtree rooted at
    $j \in C(r)$ and $n_j$ denote the size of $\tree_j$. By the inductive hypothesis, we can compute
    the representation of $g_j$ for $j \in C(r)$ in $\mathcal{O}(dn_j)$ time, since the height of the subtree rooted at 
    $j \in C(r)$ is at most $d - 1$. Further, by Proposition \ref{prop:g_pieces}, $g_j$ is represented by a 
    tuple of length at most $d - 1$. Using Proposition \ref{prop:h_form}, we can compute the representation of $h_j$ for all
    $j \in C(r)$ in $\mathcal{O}(d\sum_{j\in C(r)}n_j) = \mathcal{O}(nd)$ time, and these functions are represented by tuples of length at most $d$.

    Finally, we compute the representation of $g_r$ by applying Observation \ref{obs:g_sum} to the representations of 
    $h_j$ for all $j \in C(r)$ and the representation $(0, w_r)$ of the function $\psi \rightarrow \psi\cdot w_r$. 
    By Observation \ref{obs:g_sum_length}, the resulting tuple has 
    length at most $d$. As this step requires only $\mathcal{O}(nd)$ additions, this proves the claim.
\end{proof}

Then, by reducing the variant allele frequency $\ell_1$-regression problem to the dot product
tree labeling problem using Theorem \ref{thm:dptlp_equivalence} and applying Theorem \ref{thm:dptlp_polytime}, 
we complete the proof of Theorem \ref{thm:ellonepolytime}.

%\ellonepolytime*

\newpage
\bibliographystyle{plain}
\bibliography{references}

\newpage
\appendix
\section{Supplementary Proofs}
\label{sec:supplementary_proofs}
\clonallycanonical*
\begin{proof}
    Suppose the theorem holds for all $k < n$. Clearly, the theorem holds for the base case when $n = 1$.

    $(i) \Rightarrow (ii)$: Let $\tree$ be the $n$-clonal tree associated with $\tree$. Consider a 
    relabeling of the clones of $\tree$ by their preorder traversal index during a breadth first search
    of $\tree$ starting at the root. Then, $r(\tree)$ is assigned index $1$ and the children of $r(\tree)$ 
    are assigned indices $2, \ldots, k + 1$ where $k$ is the number of children of $r(\tree)$. Let $B'$
    be the clonal matrix associated with $\tree$ under this relabeling. Then, $B'_{i,1} = 1$
    and $B'_{1, j} = 0$ for all $j \in \{2, \ldots, n\}$ since all clones contains the mutation $1$ 
    and the root only contains the mutation $1$. Let $B_1, \ldots, B_k$ be defined recursively
    as the clonally canonical matrices associated with the subtrees rooted at the children of $r(\tree)$,
    which exist by the inductive hypothesis. Then, $B'$ is clonally canonical with blocks $B_1, \ldots, B_k$,
    completing this direction of the proof.

    $(ii) \Rightarrow (i)$: Let $\tree_1, \ldots, \tree_k$ be the clonal trees associated with the
    clonally canonical matrices $B_1, \ldots, B_k$ of $B_{2:n, 2:n}$, respectively. These 
    are guaranteed to exist by the inductive hypothesis. Then, the clonal 
    tree $\tree$ associated with $B$ is the tree with root $1$ and children $2, \ldots, k + 1$ obtained
    by attaching the roots of $\tree_1, \ldots, \tree_k$ to the root of $\tree$. This completes
    this direction of the proof.

    The equivalence of $(i)$ and $(iii)$ follows from Lemma 1 of \cite{el-kebir_reconstruction_2015},
    this completes the proof.
\end{proof}


\hform*
\begin{proof}
    If $i^* = \infty$, all slopes $s_i$ are non-negative and the function $g(\psi')$ is non-decreasing.
    Thus, the maximum of $g(\psi')$ over any interval $[\psi - 1, \psi + 1]$, is achieved at the interval's right
    most value $\psi + 1$. 

    If $i^* = -\infty$, all slopes $s_i$ are negative and the function $g(\psi')$ is strictly decreasing. 
    Thus, the maximum of $g(\psi')$ over any interval $[\psi - 1, \psi + 1]$, is achieved at the interval's left
    most value $\psi - 1$. However, since $\psi'$ is constrained to be non-negative, if $\psi < 1$ the maximum
    is achieved at $\psi' = 0$.

    If $i^* \neq \infty, -\infty$, then $g(\psi')$ is non-decreasing on the interval $[0, i^*]$ and non-increasing
    on the interval $[i^*, \infty)$. Further, the maximum of $g(\psi')$ over all non-negative $\psi'$ is bounded
    and equal to $g(i^*)$. The result then follows by a case analysis on the value of $\psi$. If $\psi$ is in $[i^* - 1, i^* + 1]$, then
    we can take $\psi' = i^*$ and achieve the maximum. If $\psi < i^* - 1$, then the function $g(\psi')$ is non-decreasing 
    on the interval $[\psi - 1, \psi + 1]$ and the maximum is achieved at the interval's right most value $\psi + 1$. 
    Symetrically, if $\psi > i^* + 1$, then the function $g(\psi')$ is non-increasing on the interval $[\psi - 1, \psi + 1]$ and the 
    maximum is achieved at the interval's left most value $\psi - 1$, which is always non-negative since $\psi > 1$. 
    As this covers all possible cases, this proves that $h$ has the form \eqref{eq:h_form}.

    To see that $h(\psi)$ is continuous, observe that
    \[\lim_{\psi \to (i^* - 1)^-} h(\psi) = g(i^*) \quad\text{and}\quad \lim_{\psi \to (i^* + 1)^+} h(\psi) = g(i^*)\]
    at the only candidates for discontinuity, $i^* - 1$ and $i^* + 1$. To see that it is concave, note that
    \[h''(\psi) = \begin{cases}
        \,0 &\text{if } \psi \in [i^* - 1, i^* + 1],\\
        \,g''(\psi + 1) &\text{if } \psi < i^* - 1,\\
        \,g''(\psi - 1) &\text{if } \psi > i^* + 1.
    \end{cases}\]
    As $g(\psi)$ is concave, its second derivative $g''(\psi)$ is non-positive, which implies that $h(\psi)$ is also concave. Since $h$ is trivially
    piecewise linear by \eqref{eq:h_form}, the proof is complete.
\end{proof}

\end{document}
